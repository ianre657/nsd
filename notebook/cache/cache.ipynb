{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: VECLIB_MAXIMUM_THREADS=1\n",
      "env: MKL_NUM_THREADS=1\n",
      "env: NUMEXPR_NUM_THREADS=1\n",
      "env: OMP_NUM_THREADS=1\n",
      "rm -rf *.o *.dSYM/ 01_skip_access 02_locality 03_matrix_matrix\r\n"
     ]
    }
   ],
   "source": [
    "%env VECLIB_MAXIMUM_THREADS=1\n",
    "%env MKL_NUM_THREADS=1\n",
    "%env NUMEXPR_NUM_THREADS=1\n",
    "%env OMP_NUM_THREADS=1\n",
    "\n",
    "!make clean\n",
    "\n",
    "import numpy as np\n",
    "#np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cache optimization\n",
    "\n",
    "1. How cache works and its importance to performance\n",
    "2. Stride analysis\n",
    "3. Tiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory hierarchy\n",
    "\n",
    "To show how cache works and its importance to performance, see table below:\n",
    "\n",
    "| Type | Latency (cycles) |\n",
    "| -- | -- |\n",
    "| CPU registers | 0 |\n",
    "| L1 cache | 4 |\n",
    "| L2 cache | 10 |\n",
    "| L3 cache | 50 |\n",
    "| Main memory | 200 |\n",
    "| Disk / drive | 100,000 |\n",
    "\n",
    "(Excerpt from Figure 6.23 in CS:APP (3/e).)\n",
    "\n",
    "Remarks:\n",
    "* Disk (or SSD) is slow.  Don't do I/O unless absolutely necessary.\n",
    "* CPU instruction is 100x faster than memory access.\n",
    "* Cache hides the memory latency.\n",
    "\n",
    "Nothing is fast without cache."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How cache works\n",
    "\n",
    "Assume we have a main memory of 64 bytes, and a cache of 8 bytes.\n",
    "\n",
    "| Access # | Decimal addr | Binary addr | Hit or miss | Cache block (binary) |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| 1 | 22 | 10110 | miss (cold) | 110 |\n",
    "| 2 | 26 | 11010 | miss (cold) | 010 |\n",
    "| 3 | 22 | 10110 | hit | 110 |\n",
    "| 4 | 26 | 11010 | hit | 010 |\n",
    "| 5 | 16 | 10000 | miss (cold) | 000 |\n",
    "| 6 | 18 | 10010 | miss (conflict) | 010 |\n",
    "| 7 | 26 | 11010 | miss (conflict) | 010 |\n",
    "\n",
    "This is a direct-map cache.  To reduce conflict misses, we may use multi-way set associativity.  2-, 4-, or 8-way set associative cache is commonly used.  Full associativity is too expensive in circuit implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory accessing speed is determined by cache block (line) size\n",
    "\n",
    "A cache block usually contains more than one byte or word.  In x86, the block (line) size is 64 bytes.  That is, when loading data from main memory to cache, it's done once a block, rather than byte by byte.\n",
    "\n",
    "```cpp\n",
    "// Sequential.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; ++i) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// Skipping 2.\n",
    "for (int i=0; i<nelem; ++i) { arr[i] = i; }\n",
    "sw.lap();\n",
    "for (int i=0; i<nelem; i+=2) { arr[i] *= 3; }\n",
    "elapsed = sw.lap();\n",
    "\n",
    "// ... 4, 8, 16, ... 1024.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 01_skip_access 01_skip_access.cpp\n",
      "Sequential takes: 0.0591595 sec\n",
      "\n",
      "Skipping 2 takes: 0.0666328 sec\n",
      "Skipping 4 takes: 0.0642925 sec\n",
      "Skipping 8 takes: 0.0616888 sec\n",
      "Skipping 16 takes: 0.0660376 sec\n",
      "\n",
      "Skipping 32 takes: 0.0452228 sec\n",
      "Skipping 64 takes: 0.0290477 sec\n",
      "Skipping 128 takes: 0.0172844 sec\n",
      "Skipping 256 takes: 0.0114255 sec\n",
      "Skipping 512 takes: 0.00685178 sec\n",
      "Skipping 1024 takes: 0.00233585 sec\n"
     ]
    }
   ],
   "source": [
    "# Show how main memory (dram) access dominates runtime.\n",
    "!make 01_skip_access ; ./01_skip_access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locality\n",
    "\n",
    "While coding we usually don't have a lot of time to do detailed cache analysis.  Instead, we keep in mind that the code runs faster when it's more compact.  This is the concept of locality.\n",
    "\n",
    "There are two kinds of locality: temporal and spatial.  Temporal locality means that a fixed address is reused in the near future.  Spatial locality means that the addresses close to the current address is reused in the near future.  The better locality, of either kind, improves the performance.  And the cache hierarchy is why locality works.\n",
    "\n",
    "To take advantage of locality, programmers analyze by using \"strides\".  A stride is the number of indexes to elements to slide when accessing the data in arrays.  The most basic striding is sequential access, or the 1-stride.  Similarly, we may have n-strides.  The larger the stride is, the worse the locality.\n",
    "\n",
    "Recall that x86 uses 64-byte cache blocks, and a double-precision floating point takes 8 bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate by matrix population\n",
    "\n",
    "To demonstrate how the data layout, i.e., majoring or striding, affects runtime, we use an example of populating $1024 \\times 1024 \\times 64$ integer elements as a matrix.  The following shapes are benchmarked (total number of elements remains unchanged):\n",
    "\n",
    "* $(1024\\times1024\\times64) \\times 1$, i.e., one-dimension\n",
    "* $(1024\\times1024\\times32) \\times 2$\n",
    "* $(1024\\times1024\\times16) \\times 4$\n",
    "* $(1024\\times1024\\times8) \\times 8$\n",
    "* $(1024\\times1024\\times4) \\times 16$\n",
    "* $(1024\\times1024\\times2) \\times 32$\n",
    "* $(1024\\times1024) \\times 64$\n",
    "* $(1024\\times512) \\times 128$\n",
    "* $(1024\\times256) \\times 256$\n",
    "* $(1024\\times128) \\times 512$\n",
    "* $(1024\\times64) \\times 1024$\n",
    "* $(1024\\times32) \\times (1024\\times2)$\n",
    "* $(1024\\times16) \\times (1024\\times4)$\n",
    "* $(1024\\times8) \\times (1024\\times8)$\n",
    "\n",
    "We populate the matrices along two axes.  First we iterate over the last index (row-major):\n",
    "\n",
    "```cpp\n",
    "// Populate by last index.\n",
    "for (size_t i=0; i<nrow; ++i) // the i-th row\n",
    "{\n",
    "    for (size_t j=0; j<ncol; ++j) // the j-th column\n",
    "    {\n",
    "        buffer[i*ncol + j] = i*ncol + j;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Then iterate over the first index (column-major):\n",
    "\n",
    "```cpp\n",
    "// Populate by first index.\n",
    "for (size_t j=0; j<ncol; ++j) // the j-th column\n",
    "{\n",
    "    for (size_t i=0; i<nrow; ++i) // the i-th row\n",
    "    {\n",
    "        buffer[i*ncol + j] = i*ncol + j;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "We will see the speed is very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 02_locality 02_locality.cpp\n",
      "# of elements: 67108864 = 67108864 x 1\n",
      "populate double flatly takes: 0.059887 sec\n",
      "populate double along last axis takes: 0.0986658 sec\n",
      "populate double along first axis takes: 0.049492 sec\n",
      "ratio: 0.501612\n",
      "\n",
      "# of elements: 67108864 = 33554432 x 2\n",
      "populate double flatly takes: 0.0491339 sec\n",
      "populate double along last axis takes: 0.0771682 sec\n",
      "populate double along first axis takes: 0.101107 sec\n",
      "ratio: 1.31022\n",
      "\n",
      "# of elements: 67108864 = 16777216 x 4\n",
      "populate double flatly takes: 0.0508462 sec\n",
      "populate double along last axis takes: 0.0543871 sec\n",
      "populate double along first axis takes: 0.20397 sec\n",
      "ratio: 3.75034\n",
      "\n",
      "# of elements: 67108864 = 8388608 x 8\n",
      "populate double flatly takes: 0.0560276 sec\n",
      "populate double along last axis takes: 0.0534111 sec\n",
      "populate double along first axis takes: 0.410397 sec\n",
      "ratio: 7.68374\n",
      "\n",
      "# of elements: 67108864 = 4194304 x 16\n",
      "populate double flatly takes: 0.0581645 sec\n",
      "populate double along last axis takes: 0.0592796 sec\n",
      "populate double along first axis takes: 0.665776 sec\n",
      "ratio: 11.2311\n",
      "\n",
      "# of elements: 67108864 = 2097152 x 32\n",
      "populate double flatly takes: 0.0615113 sec\n",
      "populate double along last axis takes: 0.0584312 sec\n",
      "populate double along first axis takes: 0.737303 sec\n",
      "ratio: 12.6183\n",
      "\n",
      "# of elements: 67108864 = 1048576 x 64\n",
      "populate double flatly takes: 0.0545041 sec\n",
      "populate double along last axis takes: 0.0521369 sec\n",
      "populate double along first axis takes: 0.793458 sec\n",
      "ratio: 15.2187\n",
      "\n",
      "# of elements: 67108864 = 524288 x 128\n",
      "populate double flatly takes: 0.0542355 sec\n",
      "populate double along last axis takes: 0.0530837 sec\n",
      "populate double along first axis takes: 0.781522 sec\n",
      "ratio: 14.7224\n",
      "\n",
      "# of elements: 67108864 = 262144 x 256\n",
      "populate double flatly takes: 0.053601 sec\n",
      "populate double along last axis takes: 0.0549364 sec\n",
      "populate double along first axis takes: 0.746615 sec\n",
      "ratio: 13.5906\n",
      "\n",
      "# of elements: 67108864 = 131072 x 512\n",
      "populate double flatly takes: 0.0527639 sec\n",
      "populate double along last axis takes: 0.0543768 sec\n",
      "populate double along first axis takes: 0.731022 sec\n",
      "ratio: 13.4436\n",
      "\n",
      "# of elements: 67108864 = 65536 x 1024\n",
      "populate double flatly takes: 0.0541765 sec\n",
      "populate double along last axis takes: 0.0528002 sec\n",
      "populate double along first axis takes: 0.721512 sec\n",
      "ratio: 13.6649\n",
      "\n",
      "# of elements: 67108864 = 32768 x 2048\n",
      "populate double flatly takes: 0.0525871 sec\n",
      "populate double along last axis takes: 0.0527919 sec\n",
      "populate double along first axis takes: 0.582407 sec\n",
      "ratio: 11.0321\n",
      "\n",
      "# of elements: 67108864 = 16384 x 4096\n",
      "populate double flatly takes: 0.0550233 sec\n",
      "populate double along last axis takes: 0.0514304 sec\n",
      "populate double along first axis takes: 0.569831 sec\n",
      "ratio: 11.0797\n",
      "\n",
      "# of elements: 67108864 = 8192 x 8192\n",
      "populate double flatly takes: 0.0509415 sec\n",
      "populate double along last axis takes: 0.0488482 sec\n",
      "populate double along first axis takes: 0.771188 sec\n",
      "ratio: 15.7874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show how striding affects population runtime.\n",
    "!make 02_locality ; ./02_locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While writing programs, it's much easier to know the stride than analyzing the cache behavior.  The latter, in many scenarios, is prohibitively difficult.\n",
    "\n",
    "Since we know the cache line is 64 byte wide, we expect the cache performance may significantly reduce when the stride is around that value (16 int elements).  As shown in the above benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition the memory and the cache before benchmarking\n",
    "\n",
    "```cpp\n",
    "// Prepopulation to cancel the effect of overcommit or delayed allocation.\n",
    "for (size_t i=0; i<nelem; ++i) { buffer[i] = nelem-i; }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array majoring in numpy\n",
    "\n",
    "Array majoring is directly related to locality.  The difference in the performance of matrix-vector multiplication is show for row- and column-majoring arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 368 ms, total: 1.51 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "float_rmajor = np.arange(dim*dim, dtype='float64').reshape((dim,dim))\n",
    "float_cmajor = float_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64 ms, sys: 1.22 ms, total: 65.2 ms\n",
      "Wall time: 63.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_float_rmajor = np.dot(float_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 130 ms, sys: 1.21 ms, total: 131 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_float_cmajor = np.dot(float_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer matrix-vector multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.12 s, sys: 388 ms, total: 1.51 s\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim = 10000\n",
    "int_rmajor = np.arange(dim*dim, dtype='int64').reshape((dim,dim))\n",
    "int_cmajor = int_rmajor.T.copy().T\n",
    "vec = np.arange(dim, dtype='int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.6 ms, sys: 968 µs, total: 71.5 ms\n",
      "Wall time: 70.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_int_rmajor = np.dot(int_rmajor, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 794 ms, sys: 1.49 ms, total: 796 ms\n",
      "Wall time: 794 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_int_cmajor = np.dot(int_cmajor, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance difference of integer arrays is much larger than floating-point arrays.  Note that `double` and `int64` both take 8 bytes.  Reason: LAPACK / MKL / vecLib.\n",
    "\n",
    "For the same reason, the floating-point multiplication is slightly faster than the integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling\n",
    "\n",
    "This is a naive implementation of matrix-matrix multiplication:\n",
    "\n",
    "```cpp\n",
    "for (size_t i=0; i<mat1.nrow(); ++i)\n",
    "{\n",
    "    for (size_t k=0; k<mat2.ncol(); ++k)\n",
    "    {\n",
    "        double v = 0;\n",
    "        for (size_t j=0; j<mat1.ncol(); ++j)\n",
    "        {\n",
    "            v += mat1(i,j) * mat2(j,k);\n",
    "        }\n",
    "        ret(i,k) = v;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The matrices are row-major.  The stride for the second matrix is `ncol2`, so it doesn't have good locality.  This naive implementation is clear, but the performance is bad.\n",
    "\n",
    "Matrix-matrix multiplication is one of the most important problems for numerical calculation, and there are many techniques available for making it fast.  Most if not all of them are about hiding the memory access latency.  Tiling is a basic technique that delivers impressive speed-up by reordering the calculation and making it cache-friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++  -std=c++17 -O3 -g -m64 -I/opt/intel/mkl/include /opt/intel/mkl/lib/libmkl_intel_lp64.a /opt/intel/mkl/lib/libmkl_sequential.a /opt/intel/mkl/lib/libmkl_core.a -lpthread -lm -ldl  -o 03_matrix_matrix 03_matrix_matrix.cpp\n",
      "Timing mkl: 0.044965 second, 1.07374 Gflo, 23.8795 Gflops\n",
      "Timing indirect: 2.85728 second, 1.07374 Gflo, 0.375791 Gflops\n",
      "Timing direct: 3.0651 second, 1.07374 Gflo, 0.350312 Gflops\n",
      "Timing tiled 32: 1.1393 second, 1.07374 Gflo, 0.942457 Gflops\n",
      "Timing tiled 64: 0.454891 second, 1.07374 Gflo, 2.36044 Gflops\n",
      "Timing tiled 128: 0.773399 second, 1.07374 Gflo, 1.38834 Gflops\n",
      "Timing tiled 256: 0.838338 second, 1.07374 Gflo, 1.2808 Gflops\n",
      "Timing tiled 512: 0.76896 second, 1.07374 Gflo, 1.39636 Gflops\n",
      "Timing tiled 1024: 0.939641 second, 1.07374 Gflo, 1.14272 Gflops\n"
     ]
    }
   ],
   "source": [
    "# Show how tiling improves runtime performance.\n",
    "!make 03_matrix_matrix ; ./03_matrix_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Consult the data sheet of one x86 CPU and one Arm CPU.  Make a table for the line size of each of the cache levels, and compare the difference between the two CPUs.\n",
    "2. Write a program that uses tiling to speed up matrix-matrix multiplication, and do not require the matrix dimension to be multiples of the tile size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Gallery of Processor Cache Effects: http://igoro.com/archive/gallery-of-processor-cache-effects/\n",
    "* Lecture Notes of Applications of Parallel Computers by David Bindel: https://www.cs.cornell.edu/~bindel/class/cs5220-s10/slides/lec03.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
